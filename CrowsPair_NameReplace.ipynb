{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGFf9yAAyj3J"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import argparse\n",
        "import difflib\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "from transformers import XLMRobertaForMaskedLM\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "import json\n",
        "import random"
      ],
      "metadata": {
        "id": "96_RjER_yxZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_excel('/content/Dataset2.xlsx')"
      ],
      "metadata": {
        "id": "gz_iOOT-y3e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_more_sentences = df1['modified_eng_sent_more'].tolist()\n",
        "eng_less_sentences = df1['modified_eng_sent_less'].tolist()\n",
        "hin_more_sentences = df1['sent_more_hindi'].tolist()\n",
        "hin_less_sentences = df1['sent_less_hindi'].tolist()\n",
        "bias_type = df1['bias_type'].tolist()\n",
        "stereo_antistereo_labels = df1['stereo_antistereo'].tolist()"
      ],
      "metadata": {
        "id": "-5KnY-ABzA5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(eng_more_sentences)"
      ],
      "metadata": {
        "id": "sWDEC0Vx9Zph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_english_male = ['Raj','Arun','Ram','Sunil','Ali','Imran','Faisal','Kabir',0]\n",
        "names_english_female = ['Rani','Sunita','Swati','Preeti','Ruhi','Sabina','Waheeda','Sultana',0]\n",
        "names_hindi_male = ['राज','अरुण','राम','सुनिल','अली','इमरान','फैसल','कबीर',0]\n",
        "names_hindi_female = ['रानी','सुनिता','स्वाती','प्रीती','रूही','सबीना','वहीदा','सुल्ताना',0 ]"
      ],
      "metadata": {
        "id": "K5BVrd5ezbeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = {}\n",
        "df_new['modified_eng_sent_more'] = []\n",
        "df_new['modified_eng_sent_less'] = []\n",
        "df_new['sent_more_hindi'] = []\n",
        "df_new['sent_less_hindi'] = []\n",
        "df_new['bias_type'] = []\n",
        "df_new['stereo_antistereo'] = []"
      ],
      "metadata": {
        "id": "I6cTZgvH0tek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_presence(sent,pattern):\n",
        "  if re.search(pattern, sent, re.IGNORECASE | re.UNICODE):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "q4nR7DjM2wa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p = r\"\\b(rahul|राहुल)\\b|\\b(rahul|राहुल)\\s\"\n",
        "# s = 'अंजली ने अपने दिन लाइफटाइम देखने और सफेद शराब के गिलास पीने में बिताए।'\n",
        "# check_presence(s,p)"
      ],
      "metadata": {
        "id": "oomNtKwn-wUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index in range(len(eng_more_sentences)):\n",
        "  eng_sent1 = eng_more_sentences[index]\n",
        "  eng_sent1 = eng_sent1.lower()\n",
        "  eng_sent2 = eng_less_sentences[index]\n",
        "  eng_sent2 = eng_sent2.lower()\n",
        "  hin_sent1 = hin_more_sentences[index]\n",
        "  hin_sent2 = hin_less_sentences[index]\n",
        "\n",
        "  pattern1 = r\"\\b(rahul|राहुल)\\b|\\b(rahul|राहुल)\\s\"\n",
        "  pattern2 = r\"\\b(anjali|अंजलि|अंजली)\\b|\\b(anjali|अंजलि|अंजली)\\s\"\n",
        "\n",
        "  name_eng_male = random.choice(names_english_male)\n",
        "  corresponding_index = names_english_male.index(name_eng_male)\n",
        "  name_hin_male = names_hindi_male[corresponding_index]\n",
        "\n",
        "  name_eng_female = random.choice(names_english_female)\n",
        "  corresponding_index = names_english_female.index(name_eng_female)\n",
        "  name_hin_female = names_hindi_female[corresponding_index]\n",
        "\n",
        "  if check_presence(eng_sent1,pattern1) and check_presence(hin_sent1,pattern1):\n",
        "    name_pattern_rahul1 = re.compile(r'rahul')\n",
        "    name_pattern_rahul2= re.compile(r'राहुल')\n",
        "\n",
        "    # name_eng = random.choice(names_english_male)\n",
        "    # corresponding_index = names_english_male.index(name_eng)\n",
        "    # name_hin = names_hindi_male[corresponding_index]\n",
        "\n",
        "    if name_eng_male == 0:\n",
        "      new_eng_sent1 = eng_sent1\n",
        "      new_hin_sent1 = hin_sent1\n",
        "    else:\n",
        "      new_eng_sent1 = name_pattern_rahul1.sub(lambda _:  name_eng_male, eng_sent1)\n",
        "      new_hin_sent1 = name_pattern_rahul2.sub(lambda _:  name_hin_male, hin_sent1)\n",
        "      #print(new_eng_sent1)\n",
        "      #print(new_hin_sent1)\n",
        "\n",
        "  elif check_presence(eng_sent1,pattern2) and check_presence(hin_sent1,pattern2):\n",
        "    name_pattern_anjali1 = re.compile(r'anjali')\n",
        "    name_pattern_anjali2= re.compile(r'अंजली')\n",
        "\n",
        "    # name_eng = random.choice(names_english_female)\n",
        "    # corresponding_index = names_english_female.index(name_eng)\n",
        "    # name_hin = names_hindi_female[corresponding_index]\n",
        "\n",
        "    if name_eng_female == 0:\n",
        "      new_eng_sent1 = eng_sent1\n",
        "      new_hin_sent1 = hin_sent1\n",
        "    else:\n",
        "      new_eng_sent1 = name_pattern_anjali1.sub(lambda _:  name_eng_female, eng_sent1)\n",
        "      new_hin_sent1 = name_pattern_anjali2.sub(lambda _:  name_hin_female, hin_sent1)\n",
        "      print(new_eng_sent1)\n",
        "      print(new_hin_sent1)\n",
        "\n",
        "  else:\n",
        "    new_eng_sent1 = eng_sent1\n",
        "    new_hin_sent1 = hin_sent1\n",
        "\n",
        "  if check_presence(eng_sent2,pattern1) and check_presence(hin_sent2,pattern1):\n",
        "    name_pattern_rahul1 = re.compile(r'rahul')\n",
        "    name_pattern_rahul2= re.compile(r'राहुल')\n",
        "\n",
        "    # name_eng = random.choice(names_english_male)\n",
        "    # corresponding_index = names_english_male.index(name_eng)\n",
        "    # name_hin = names_hindi_male[corresponding_index]\n",
        "\n",
        "    if name_eng_male == 0:\n",
        "      new_eng_sent2 = eng_sent2\n",
        "      new_hin_sent2 = hin_sent2\n",
        "    else:\n",
        "      new_eng_sent2 = name_pattern_rahul1.sub(lambda _:  name_eng_male, eng_sent2)\n",
        "      new_hin_sent2 = name_pattern_rahul2.sub(lambda _:  name_hin_male, hin_sent2)\n",
        "      #print(new_eng_sent2)\n",
        "      #print(new_hin_sent2)\n",
        "\n",
        "  elif check_presence(eng_sent2,pattern2) and check_presence(hin_sent2,pattern2):\n",
        "    name_pattern_anjali1 = re.compile(r'anjali')\n",
        "    name_pattern_anjali2= re.compile(r'अंजली')\n",
        "\n",
        "    # name_eng = random.choice(names_english_female)\n",
        "    # corresponding_index = names_english_female.index(name_eng)\n",
        "    # name_hin = names_hindi_female[corresponding_index]\n",
        "\n",
        "    if name_eng_female == 0:\n",
        "      new_eng_sent2 = eng_sent2\n",
        "      new_hin_sent2 = hin_sent2\n",
        "    else:\n",
        "      new_eng_sent2 = name_pattern_anjali1.sub(lambda _:  name_eng_female, eng_sent2)\n",
        "      new_hin_sent2 = name_pattern_anjali2.sub(lambda _:  name_hin_female, hin_sent2)\n",
        "      print(new_eng_sent2)\n",
        "      print(new_hin_sent2)\n",
        "\n",
        "  else:\n",
        "    new_eng_sent2 = eng_sent2\n",
        "    new_hin_sent2 = hin_sent2\n",
        "\n",
        "\n",
        "  df_new['modified_eng_sent_more'].append(new_eng_sent1)\n",
        "  df_new['modified_eng_sent_less'].append(new_eng_sent2)\n",
        "  df_new['sent_more_hindi'].append(new_hin_sent1)\n",
        "  df_new['sent_less_hindi'].append(new_hin_sent2)\n",
        "  df_new['bias_type'].append(bias_type[index])\n",
        "  df_new['stereo_antistereo'].append(stereo_antistereo_labels[index])\n",
        "  #print(index, \"here\")"
      ],
      "metadata": {
        "id": "a4ROdiQZ1Jks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_new['stereo_antistereo'])"
      ],
      "metadata": {
        "id": "-bSwTrp79PYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Dataset2_names.pickle', 'wb') as handle:\n",
        "    pickle.dump(df_new, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "apFV4B7H6s46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.DataFrame(df_new)"
      ],
      "metadata": {
        "id": "YIKDK2YB8SVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.head(25)"
      ],
      "metadata": {
        "id": "c6FqHYDx8YS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.to_excel('Dataset2_names.xlsx', index=False)"
      ],
      "metadata": {
        "id": "kUEWx2Xz8ZuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------"
      ],
      "metadata": {
        "id": "z6mkzCZxFpsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "INh01K6-jme-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import argparse\n",
        "import difflib\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "from transformers import XLMRobertaForMaskedLM\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "import json\n",
        "import random"
      ],
      "metadata": {
        "id": "JXSx7ugEjoTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/bernice\")\n",
        "model = XLMRobertaForMaskedLM.from_pretrained('jhu-clsp/bernice')"
      ],
      "metadata": {
        "id": "l6ElMjahjOcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_token = tokenizer.mask_token\n",
        "mask_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
        "softmax = torch.nn.Softmax(dim=0)"
      ],
      "metadata": {
        "id": "VW9x4DKqwYH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity_dic = {}\n",
        "identity_num_tokens = {}\n",
        "identity_token_ids = {}\n",
        "identity_dic[\"Age\"]= [\"young people\", \"old people\"]\n",
        "identity_dic[\"Caste\"]= [\"brahmins\", \"kshatriyas\", \"dalits\", \"vaishyas\"]\n",
        "identity_dic[\"Gender\"]= [\"females\", \"males\"]\n",
        "identity_dic[\"Physical Appearance1\"]= [\"fat people\", \"slim people\"]\n",
        "identity_dic[\"Physical Appearance2\"]= [\"dark-skinned people\", \"fair-skinned people\"]\n",
        "identity_dic[\"Religion\"]= [\"buddhists\", \"christians\", \"hindus\", \"jains\", \"muslims\", \"sikhs\"]"
      ],
      "metadata": {
        "id": "Y2vcdec48mPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for identity_group,identity_terms in identity_dic.items():\n",
        "  identity_num_tokens[identity_group] = []\n",
        "  identity_token_ids[identity_group] = []\n",
        "\n",
        "  for term in identity_terms:\n",
        "    tokens = tokenizer.encode(term, return_tensors='pt')[0]\n",
        "    tokens = tokens[1:-1].tolist()\n",
        "    identity_num_tokens[identity_group].append(len(tokens))\n",
        "    identity_token_ids[identity_group].append(tokens)\n",
        "\n",
        "for identity_group in identity_num_tokens.keys():\n",
        "  print(\"-\"*50)\n",
        "  print(identity_group)\n",
        "  print(identity_dic[identity_group])\n",
        "  print(identity_num_tokens[identity_group])\n",
        "  print(identity_token_ids[identity_group])\n"
      ],
      "metadata": {
        "id": "4k-NpegrfBhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_index_span(tokenizer,sent_words,sent_token_ids):\n",
        "  for word in sent_words:\n",
        "\n",
        "    if word[-1] == ',' or word[-1] == '।' or word[-1] == '.':\n",
        "      word = word[:len(word)-1]\n",
        "\n",
        "    tokens = tokenizer.encode(word, return_tensors='pt')[0]\n",
        "    tokens_subset = tokens[1:-1]\n",
        "    #print(tokens_subset)\n",
        "\n",
        "    index = torch.nonzero(torch.isin(sent_token_ids[0] , tokens_subset))\n",
        "    index = index.squeeze()\n",
        "\n",
        "    if index.numel() == 1:\n",
        "      index = [index.item()]\n",
        "    else:\n",
        "      index = index.tolist()\n",
        "    # print(index)\n",
        "    # print(word)\n",
        "    # print(\"-\"*50)\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "JihBed-MtqzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#template = '[SLOT] often tend to be @.'\n",
        "identity_group = 'age'\n",
        "template = 'old people often tend to be aggresive.'\n",
        "sent_token_ids = tokenizer.encode(template, return_tensors='pt')\n",
        "id_term = ['old people']\n",
        "mask_idxs = word_index_span(tokenizer,id_term,sent_token_ids)\n",
        "print(mask_idxs)"
      ],
      "metadata": {
        "id": "ahymoMIBk56M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_masked_token_ids = sent_token_ids.clone()\n",
        "for mask_idx in mask_idxs:\n",
        "  sent_masked_token_ids[0][mask_idx] = mask_id"
      ],
      "metadata": {
        "id": "UC9lXc28t-l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(sent_masked_token_ids)\n",
        "hidden_states = output[0].squeeze(0)"
      ],
      "metadata": {
        "id": "PrWWJ0RXwlPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probability = {}"
      ],
      "metadata": {
        "id": "Ms7tWy25xX8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for mask_idx in mask_idxs:\n",
        "  hs = hidden_states[mask_idx]\n",
        "  hs = softmax(hs)\n"
      ],
      "metadata": {
        "id": "QvymoTPtxAg9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}